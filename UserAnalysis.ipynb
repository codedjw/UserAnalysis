{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import pandas as pd\n",
    "# 数据库访问\n",
    "import MySQLdb, datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取当前路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "#获取脚本文件的当前路径\n",
    "def cur_file_dir():\n",
    "    #获取脚本路径\n",
    "    path = sys.path[0]\n",
    "    #判断为脚本文件还是py2exe编译后的文件，如果是脚本文件，则返回的是脚本的目录，如果是py2exe编译后的文件，则返回的是编译后的文件路径\n",
    "    if os.path.isdir(path):\n",
    "        return path\n",
    "    elif os.path.isfile(path):\n",
    "        return os.path.dirname(path)\n",
    "#打印结果\n",
    "\n",
    "# cur_file_dir = cur_file_dir() + '/'\n",
    "cur_file_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据（user_dim.txt）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 读取数据（user_dim.txt）\n",
    "X = np.loadtxt('/Users/dujiawei/git/UserAnalysis/result/user_dim.txt')\n",
    "\n",
    "##归一化\n",
    "# nrow, _ = X.shape\n",
    "# for row in xrange(nrow):\n",
    "#     X[row] = X[row]/sum(X[row])\n",
    "# X = np.vectorize(lambda x: round(x,2))(X)\n",
    "\n",
    "# Open database connection\n",
    "conn = MySQLdb.connect(host='localhost',user='root',passwd='',db='qyw', charset='utf8')\n",
    "#c = ['yellowgreen', 'lightskyblue', 'lightcoral', 'gold', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE算法降维+DBSCAN聚类（效果最好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0 10  1  2 -1  2  1  1  0  0 -1  1  3  0  0  1  4  3  4  0  4  1\n",
      "  4  0  0  2  0  0  0  2  1 -1  0  0  0 -1 -1  4  0  0  5  4  0  0  0  0  0\n",
      " -1 -1 -1 -1  1  1  2  6  7  2  1 10  0  0  3  1  0  3 10  0  0  2  0  4  0\n",
      "  0  0  1  0  6  8  4  0  1  0  0  2  1  4  1 -1  0  8  0  0  0 -1  7  2  0\n",
      "  8  1  4  8  0  0  1 -1  0  0  2  6  2  0  2  0  9  2  4  0  6  2  6  5  0\n",
      " -1  2  0  3  0  0 -1 -1 -1  7  0  0  0  1  4  0  0  5  1 -1  2  6  0  0  0\n",
      " -1  0  0  4  0 -1 -1  7  0  2  4  2  6  2  2  0  0  7  1 -1  5  7  1  0  2\n",
      "  0  0  6  0  0  1  1  0  0  4  0  0 -1  2  1  2 -1  5  0  0  4 10  2  7  0\n",
      "  6  4  6  4  0  1  6  0  5 -1  0  9  2  4  4  4  9  0  1  2  1  5  8  8  2\n",
      "  1  1  1  0  9  6  0  0 10  7 10 -1  3  0  1  4  4  0  9  0 -1  0  0  4  0\n",
      "  1  4  1  9  2  0  2  4  0  1  0  1  0  6  0  0  1 -1  2  4  0 -1  0  0  2\n",
      "  4  1  1  4  0  0  4  0 -1  6  6  4 10  9  4  4  0  0  0  7 -1  0  1  1  4\n",
      "  3  0  0  1  2  0  0  0  1  4  2  4  0  0  2  0  0  4  0  4  4  0  0  4  0\n",
      "  0  1  2  0  1  0  2  1 10 -1  0  0  0  2  0 -1  0  1  6  1  8  4  0  0 -1\n",
      "  0  0  8  0  7  0  2  2  3 -1  0  0  9 -1  0  0  0  0  0 -1  4  2  1  6  0\n",
      "  4  2  0  9  9  6 -1  1  0  1  4 -1  2 10  7  0  0  0  3 -1 10  0  9  7  4\n",
      "  0  0  2  0  0  7  0  1  5  0  4  0  6  0  0 10  4  9  7  2  5  0  0  4  0\n",
      "  4 10 10  1  0  1  2 10  0  8  3  0  0  0 -1  1  1  6  1  2  6 10  2  1  4\n",
      "  3  1 -1  0  0  6 -1  0 10 -1  2  1  0  4  0  8  4  7  4  0 -1  0 -1  7  0\n",
      "  2  1  2  0  0  4  0  0  2  0 -1  1  1  0  0  2  4  2  4  0  1  2  4  3  0\n",
      "  5  4  0  8  0  2  2  0  1  0  4  0  0  4 10  0 -1  0  0  0  4  0  6  1  5\n",
      "  3  1  2  4  0  0  1  0 -1  0  1  7  0  0  0  4  0  7  7  0  4  2  0  0  3\n",
      "  4  0  6  0  4  0  1 10  1  0 -1  2  1  0  1  0  2  1  0  4  3  0  2  3 10\n",
      "  0 -1  2  1  4  2  1  1  5  1  4  1  0  0  0  2  4  1  1  1  1  0  9  5  0\n",
      "  3  2  2  1 -1  0 -1  2  0  0  6  2  7  8  7 -1  3  0  2  0  6  0  0 10  3\n",
      "  0 -1  8  0  1  0  1  0  2 -1  0  0  5 -1  3  0  5  4  3  0  7  0  0  3  4\n",
      "  3  0 -1  0  4  0  5  0  6  1  1  1  2  1 10  1  0  2  2  4  4  0  7  2  0\n",
      "  0  1  1  1  9  8  0  0  1  3  0  0  3 -1  1  7  0  6  0  0  2  2  2  1  0\n",
      "  0  1  2  0  1  4 -1  0  4  8  8  6  0  0  4  8  0  2  5  5  0  0  0  2  0\n",
      "  4  0  0  4 10  1  3  1  1  2  4  4  0  0  0  0  0  0  0  2  2 -1  0  0  0\n",
      "  2  0  6  7  0  4  0  4  1  2  6  7 10  1  2  0  0  2  1  2  2  0  1  0 -1\n",
      "  3  4  0  0 10  0  2  4  0  0  0  0 -1  0  1 -1  4  2  0  0  0  5  7 -1 -1\n",
      " -1 -1 -1  0  1  1  4  0  1  0  1  0  4  4  1  0  0  3  0  9  1  2 -1 10 10\n",
      "  4  4  4  6  4  4  2  0  6  0  6  9  0 10  6  0  4  8  4  6  0  0  4  2  0\n",
      "  7  5  4  1  0  5  1 10  4  7  2 10  5  0  0  6  1  4  0  0  3  8  1  0  3\n",
      "  4  0  1  0  9  0  2  5  0  0  2  0 -1 10  0  5  0 -1  1  1  7  3 -1  0  0\n",
      " -1  0 -1 -1  1  2  0  0  0 -1  1 -1  0  4  1  1  1 10  0  2  7  1  0  5  6\n",
      " -1  2  0  0 -1  7  0  1  6  0  3  4  4 -1  0  4  0 -1  6  2  7  7  0  4  6\n",
      "  0  0  0 -1 -1  2  0  1  0  9  0  1  6 10  0  2  0  0  8  9  2  6  0 10  4\n",
      "  0 -1  0  0  0  0  0  4  0  0  3  0  0  0  8 -1  0 10  2  6  0  5  2  0  0]\n",
      "11\n",
      "915\n",
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## t-SNE算法降维+DBSCAN聚类（效果最好）\n",
    "# ###############################################################################\n",
    "# not good\n",
    "# Compute Kmeans\n",
    "# Visualize the results on PCA-reduced data\n",
    "#tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "#reduced_data = tsne.fit_transform(X)\n",
    "#plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "#plt.savefig(cur_file_dir+'result/'+'user_dr_tsne.png')\n",
    "## plt.show()\n",
    "#plt.cla()\n",
    "#plt.clf()\n",
    "#plt.close()\n",
    "## compute K-means\n",
    "#kmeans = KMeans(init='k-means++', n_clusters=4, n_init=1)\n",
    "#cls = kmeans.fit(reduced_data)\n",
    "#print cls.cluster_centers_\n",
    "#print cls.labels_\n",
    "#print cls.inertia_\n",
    "#labels = cls.labels_\n",
    "\n",
    "###############################################################################\n",
    "## Visualize the results on PCA-reduced data\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "reduced_data = tsne.fit_transform(X)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "plt.savefig(cur_file_dir+'result/'+'user_dr_tsne.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# Compute DBSCAN\n",
    "#D = distance.squareform(distance.pdist(X)) # 高维数据\n",
    "D = distance.squareform(distance.pdist(reduced_data)) # 低维数据\n",
    "D = np.sort(D,axis=0)\n",
    "minPts = 20\n",
    "nearest = D[1:(minPts+1), :]\n",
    "nearest = nearest.reshape(1, nearest.size)\n",
    "sort_nearest = np.sort(nearest)\n",
    "plt.plot(range(len(sort_nearest[0,:])), sort_nearest[0,:], linewidth=1.0, marker='x')\n",
    "#plt.axis([-2, len(sort_nearest[0,:])+1000, -2, max(sort_nearest[0,:])+2])\n",
    "plt.savefig(cur_file_dir+'result/'+'nearest.png')\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "#db = DBSCAN(eps=0.90, min_samples=minPts).fit(X) # 高维数据\n",
    "db = DBSCAN(eps=5, min_samples=minPts).fit(reduced_data) # 低维数据\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print labels\n",
    "print n_clusters_\n",
    "print len(labels[labels>=0])\n",
    "\n",
    "## 散点图\n",
    "#colors = [c[int(i) % len(c)] for i in labels]\n",
    "#colors = labels\n",
    "#plt.scatter(reduced_data[:, 0], reduced_data[:, 1], 20, colors) # 20为散点的直径\n",
    "\n",
    "## 图形展示\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0,1,len(unique_labels)))\n",
    "f = open(cur_file_dir+'result/cls_special_user_dim.txt', 'w')\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k  == -1:\n",
    "        # 噪声显示为黑色\n",
    "        col = 'k'\n",
    "        markersize = 3\n",
    "    else:\n",
    "        markersize = 8\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = reduced_data[class_member_mask]\n",
    "    if k >= 0: # 非离群点\n",
    "        xxy = X[class_member_mask]\n",
    "        nrow,ncol = xxy.shape\n",
    "        np.savetxt(f, xxy[random.randint(0, nrow-1),:].reshape(1,ncol),fmt='%d')\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,   \n",
    "             markeredgecolor='k', markersize=markersize)   \n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "f.close()\n",
    "plt.savefig(cur_file_dir+'result/'+'user_cluster_dbscan.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "## 展示用户聚类结果（散点图和excel表格）\n",
    "USERS = np.loadtxt('/Users/dujiawei/git/UserAnalysis/result/user.txt')\n",
    "USERS_CLS =  np.hstack((USERS.reshape(USERS.size, 1),labels.reshape(labels.size,1)))\n",
    "print USERS_CLS.shape\n",
    "np.savetxt(cur_file_dir+'result/user_cls.txt', USERS_CLS, fmt='%d')\n",
    "USERS_CLS_DF = pd.DataFrame(USERS_CLS, columns=['USER_ID', 'CLS_ID'])\n",
    "print USERS_CLS_DF.shape\n",
    "USERS_CLS_DF.to_sql('qyw_7th_user_clusters', conn, flavor='mysql', if_exists='replace', index=False)\n",
    "USERS_CLS_CNTCASE = pd.read_sql('''\n",
    "    SELECT t1.*,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT USER_ID,\n",
    "          COUNT(DISTINCT CASE_ID) AS CNT_CASE\n",
    "   FROM qyw_7th_yy_succ_all_selected\n",
    "   GROUP BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters) AS t2 ON t1.USER_ID = t2.USER_ID\n",
    "ORDER BY t1.CNT_CASE DESC;\n",
    "''', con=conn)\n",
    "#colors = [c[int(i) % len(c)] for i in np.array(USERS_CLS_CNTCASE['CLS_ID'])]\n",
    "#colors = USERS_CLS_CNTCASE['CLS_ID']\n",
    "#plt.scatter(USERS_CLS_CNTCASE['CNT_CASE'], USERS_CLS_CNTCASE['USER_ID'], 20, colors) # 20为散点的直径\n",
    "## 图形展示\n",
    "labels = np.array(USERS_CLS_CNTCASE['CLS_ID'])\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0,1,len(unique_labels)))\n",
    "cntcases = np.array(USERS_CLS_CNTCASE['CNT_CASE'])\n",
    "cntcases = cntcases.reshape(cntcases.size, 1)\n",
    "uids = np.array(USERS_CLS_CNTCASE['USER_ID'])\n",
    "uids = uids.reshape(uids.size, 1)\n",
    "data = np.hstack((cntcases, uids))\n",
    "plt.axis([min(cntcases)-2, max(cntcases)+2, min(uids)-1000000, max(uids)+1000000]) #hard code, +- 为x轴、y轴分割（一格）单位\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k  == -1:\n",
    "        # 噪声显示为黑色\n",
    "        col = 'k'\n",
    "        markersize = 3\n",
    "    else:\n",
    "        markersize = 8\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = data[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,   \n",
    "             markeredgecolor='k', markersize=markersize) \n",
    "plt.savefig(cur_file_dir+'result/'+'user_cls_cntcase.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "USERS_CLS_CNTCASE.to_excel(cur_file_dir+'result/'+'user_cls_cntcase.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 年龄分布（年龄映射为标称属性）optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AGE_ALL = pd.read_sql('''\n",
    "# SELECT AGE FROM qyw_7th_user;\n",
    "# ''',con=conn)\n",
    "# AGE_ALL = AGE_ALL.AGE\n",
    "# AGE_ALL = AGE_ALL.fillna(-1)\n",
    "# AGE_ALL = AGE_ALL.sort_values()\n",
    "# plt.scatter(range(AGE_ALL.size),AGE_ALL)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印分类树规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lineage(tree, feature_names, class_names, numerial_feature_names=None):\n",
    "    left= tree.tree_.children_left\n",
    "    right= tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    values = tree.tree_.value\n",
    "\n",
    "    # get ids of child nodes\n",
    "    idx = np.argwhere(left == -1)[:,0]     \n",
    "\n",
    "    def recurse(left, right, child, lineage=None):          \n",
    "        if lineage is None:\n",
    "            lineage = [child]\n",
    "        if child in left:\n",
    "            parent = np.where(left == child)[0].item()\n",
    "            split = 'l'\n",
    "        else:\n",
    "            parent = np.where(right == child)[0].item()\n",
    "            split = 'r'\n",
    "\n",
    "        lineage.append((parent, split, threshold[parent], features[parent]))\n",
    "\n",
    "        if parent == 0:\n",
    "            lineage.reverse()\n",
    "            return lineage\n",
    "        else:\n",
    "            return recurse(left, right, parent, lineage)\n",
    "    f = open(cur_file_dir+'result/user_info_cls_rules.txt','w')\n",
    "    i = 0\n",
    "    for child in idx:\n",
    "        str = ''\n",
    "        for node in recurse(left, right, child):\n",
    "            if isinstance(node, tuple):\n",
    "                _,cur_split,cur_threshold,cur_feature = node\n",
    "                feature_label = cur_feature\n",
    "                if numerial_feature_names:\n",
    "                    if cur_feature in numerial_feature_names:\n",
    "                        comp_label = '<='\n",
    "                        if cur_split == 'r':\n",
    "                            comp_label = '>'\n",
    "                        feature_label = '{lnum}{comp}{thr}'.format(lnum=feature_label, comp=comp_label, thr=cur_threshold)\n",
    "                and_label = ''\n",
    "                if str != '':\n",
    "                    and_label = ' AND '\n",
    "                not_label = ''\n",
    "                if cur_split == 'l':\n",
    "                    not_label = 'NOT '\n",
    "                    if numerial_feature_names:\n",
    "                        if cur_feature in numerial_feature_names:\n",
    "                            not_label = ''\n",
    "                str = '{origin}{land}({lnot}{lnew})'.format(origin=str, land=and_label, lnot=not_label, lnew=feature_label)\n",
    "            else:\n",
    "                value = values[node]\n",
    "                max_idx = 0\n",
    "                max_n_samples = 0\n",
    "                _, v_ncol = value.shape\n",
    "                for v in xrange(v_ncol):\n",
    "                    if max_n_samples < value[0,v]:\n",
    "                        max_n_samples = value[0,v]\n",
    "                        max_idx = v\n",
    "                cur_class = class_names[max_idx]\n",
    "                str = '{lclass}\\t{features}'.format(lclass=cur_class, features=str)\n",
    "#                 print str\n",
    "                f.write(str)\n",
    "                if i<len(idx)-1:\n",
    "                    f.write('\\n')\n",
    "                i = i+1\n",
    "                str = ''\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取（分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGE=0 TO 10', 'AGE=10 TO 20', 'AGE=20 TO 30', 'AGE=30 TO 40', 'AGE=40 TO 50', 'AGE=50 TO 60', 'AGE=60 TO 70', 'AGE=70 TO 80', 'AGE=80 TO 90', 'AGE=90 TO 100', 'AGE=UNKNOWN', 'CITY=HOSPITAL LOCATED', 'CITY=OTHERS', 'GENDER=FEMALE', 'GENDER=MALE', 'GENDER=UNKNOWN', 'USER_TYPE=MEDICAL_GUIDED', 'USER_TYPE=SELF_GUIDED']\n",
      "['cls_#0', 'cls_#1', 'cls_#2', 'cls_#3', 'cls_#4', 'cls_#5', 'cls_#6', 'cls_#7', 'cls_#8', 'cls_#9', 'cls_#10']\n"
     ]
    }
   ],
   "source": [
    "USER_INFO_CLS = pd.read_sql('''\n",
    "SELECT t1.GENDER,\n",
    "       t1.MEDICAL_GUIDE AS USER_TYPE,\n",
    "       t1.CITY,\n",
    "       t1.AGE,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user\n",
    "   ORDER BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters\n",
    "   WHERE CLS_ID >= 0\n",
    "   ORDER BY USER_ID) AS t2 ON t1.USER_ID = t2.USER_ID;''', con=conn)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 0, 'GENDER'] = 'UNKNOWN'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 1, 'GENDER'] = 'MALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 2, 'GENDER'] = 'FEMALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] != '', 'USER_TYPE'] = 'MEDICAL_GUIDED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] == '', 'USER_TYPE'] = 'SELF_GUIDED'\n",
    "USER_INFO_CLS['AGE'].fillna(-1, inplace=True)\n",
    "USER_INFO_CLS['CITY'].fillna(u'未知', inplace=True)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] != u'武汉', 'CITY'] = 'HOSPITAL LOCATED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] == u'武汉', 'CITY'] = 'OTHERS'\n",
    "\n",
    "# 年龄离散化（optional）\n",
    "USER_INFO_CLS.AGE = USER_INFO_CLS.AGE.apply(lambda x:'{begin} TO {end}'.format(begin=int(x)/10*10,end=(int(x)/10+1)*10))\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['AGE'] == '-10 TO 0', 'AGE'] = 'UNKNOWN'\n",
    "\n",
    "# encoding\n",
    "USER_INFO_CLS_FEA = USER_INFO_CLS.drop(['CLS_ID'], axis=1)\n",
    "USER_INFO_CLS_FEA_DICT = USER_INFO_CLS_FEA.T.to_dict().values()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "USER_INFO_CLS_DATA = vec.fit_transform(USER_INFO_CLS_FEA_DICT).toarray()\n",
    "USER_INFO_CLS_FEA_NAMES = vec.get_feature_names()\n",
    "print USER_INFO_CLS_FEA_NAMES\n",
    "USER_INFO_CLS_TARGET = USER_INFO_CLS.CLS_ID.values\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(USER_INFO_CLS_DATA, USER_INFO_CLS_TARGET)\n",
    "\n",
    "from sklearn.externals.six import StringIO \n",
    "import pydot \n",
    "\n",
    "# dot_data = StringIO() \n",
    "# tree.export_graphviz(clf, out_file=dot_data) \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "# graph.write_png(cur_file_dir+\"result/USER_INFO_CLS_clf_1.png\") \n",
    "\n",
    "USER_INFO_CLS_TARGET = ['cls_#'+str(int(i)) for i in np.unique(USER_INFO_CLS_TARGET)]\n",
    "print USER_INFO_CLS_TARGET\n",
    "from IPython.display import Image  \n",
    "dot_data = StringIO()  \n",
    "tree.export_graphviz(clf, out_file=dot_data,  \n",
    "                         feature_names=USER_INFO_CLS_FEA_NAMES,  \n",
    "                         class_names=USER_INFO_CLS_TARGET,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=False)  # (default=False) When set to False, ignore special characters for PostScript compatibility.\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png(cur_file_dir+\"result/user_info_cls_clf.png\")\n",
    "\n",
    "get_lineage(clf, USER_INFO_CLS_FEA_NAMES, USER_INFO_CLS_TARGET) # 年龄离散化（optional）\n",
    "# get_lineage(clf, USER_INFO_CLS_FEA_NAMES, USER_INFO_CLS_TARGET, ['AGE'])\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取（预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 25\n",
      "----svm_c_rbf----\n",
      "svm_c_rbf Score: 0.80\n",
      "svm_c_rbf Cross Avg. Score: 0.76 (+/- 0.16)\n",
      "svm_c_rbf Time: 0.01\n",
      "----tree----\n",
      "tree Score: 0.72\n",
      "tree Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "tree Time: 0.01\n",
      "----forest_10----\n",
      "forest_10 Score: 0.80\n",
      "forest_10 Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "forest_10 Time: 0.14\n",
      "----forest_100----\n",
      "forest_100 Score: 0.80\n",
      "forest_100 Cross Avg. Score: 0.80 (+/- 0.25)\n",
      "forest_100 Time: 1.35\n",
      "----svm_linear----\n",
      "svm_linear Score: 0.80\n",
      "svm_linear Cross Avg. Score: 0.80 (+/- 0.25)\n",
      "svm_linear Time: 0.02\n",
      "----bayes----\n",
      "bayes Score: 0.76\n",
      "bayes Cross Avg. Score: 0.68 (+/- 0.41)\n",
      "bayes Time: 0.01\n",
      "----svm_c_linear----\n",
      "svm_c_linear Score: 0.80\n",
      "svm_c_linear Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "svm_c_linear Time: 0.02\n"
     ]
    }
   ],
   "source": [
    "USER_INFO_CLS = pd.read_sql('''\n",
    "SELECT t1.GENDER,\n",
    "       t1.MEDICAL_GUIDE AS USER_TYPE,\n",
    "       t1.CITY,\n",
    "       t1.AGE,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user\n",
    "   ORDER BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters\n",
    "   WHERE CLS_ID >= 0\n",
    "   ORDER BY USER_ID) AS t2 ON t1.USER_ID = t2.USER_ID;''', con=conn)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 0, 'GENDER'] = 'UNKNOWN'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 1, 'GENDER'] = 'MALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 2, 'GENDER'] = 'FEMALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] != '', 'USER_TYPE'] = 'MEDICAL_GUIDED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] == '', 'USER_TYPE'] = 'SELF_GUIDED'\n",
    "USER_INFO_CLS['AGE'].fillna(-1, inplace=True)\n",
    "USER_INFO_CLS['CITY'].fillna(u'未知', inplace=True)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] != u'武汉', 'CITY'] = 'HOSPITAL LOCATED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] == u'武汉', 'CITY'] = 'OTHERS'\n",
    "\n",
    "# # 年龄离散化（optional）\n",
    "# USER_INFO_CLS.AGE = USER_INFO_CLS.AGE.apply(lambda x:'{begin} TO {end}'.format(begin=int(x)/10*10,end=(int(x)/10+1)*10))\n",
    "# USER_INFO_CLS.loc[USER_INFO_CLS['AGE'] == '-10 -> 0', 'AGE'] = 'UNKNOWN'\n",
    "\n",
    "# encoding\n",
    "USER_INFO_CLS_FEA = USER_INFO_CLS.drop(['CLS_ID'], axis=1)\n",
    "USER_INFO_CLS_FEA_DICT = USER_INFO_CLS_FEA.T.to_dict().values()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "USER_INFO_CLS_DATA = vec.fit_transform(USER_INFO_CLS_FEA_DICT).toarray()\n",
    "USER_INFO_CLS_FEA_NAMES = vec.get_feature_names()\n",
    "# print USER_INFO_CLS_FEA_NAMES\n",
    "USER_INFO_CLS_TARGET = USER_INFO_CLS.CLS_ID.values\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(USER_INFO_CLS_DATA, USER_INFO_CLS_TARGET)\n",
    "print len(data_train), len(data_test)\n",
    "\n",
    "# 这里选择朴素贝叶斯、决策树、随机森林和SVM来做一个对比。\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import datetime\n",
    "estimators = {}\n",
    "estimators['bayes'] = GaussianNB()\n",
    "estimators['tree'] = tree.DecisionTreeClassifier()\n",
    "estimators['forest_100'] = RandomForestClassifier(n_estimators = 100)\n",
    "estimators['forest_10'] = RandomForestClassifier(n_estimators = 10)\n",
    "estimators['svm_c_rbf'] = svm.SVC()\n",
    "estimators['svm_c_linear'] = svm.SVC(kernel='linear')\n",
    "estimators['svm_linear'] = svm.LinearSVC()\n",
    "# estimators['svm_nusvc'] = svm.NuSVC() # error, because: http://stackoverflow.com/questions/26987248/nu-is-infeasible\n",
    "\n",
    "for k in estimators.keys():\n",
    "    start_time = datetime.datetime.now()\n",
    "    print '----%s----' % k\n",
    "    estimators[k] = estimators[k].fit(data_train, target_train)\n",
    "    pred = estimators[k].predict(data_test)\n",
    "    # print target_test vs. pred\n",
    "#     print target_test\n",
    "#     print 'vs. '\n",
    "#     print pred\n",
    "    # This is predict score\n",
    "    print(\"%s Score: %0.2f\" % (k, estimators[k].score(data_test, target_test)))\n",
    "    scores = cross_validation.cross_val_score(estimators[k], data_test, target_test, cv=5)\n",
    "    print(\"%s Cross Avg. Score: %0.2f (+/- %0.2f)\" % (k, scores.mean(), scores.std() * 2))\n",
    "#     print(\"%s Pred Score: %0.2f\" % (k, ((float)(len(target_test[target_test == pred])))/len(target_test)))\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_spend = end_time - start_time\n",
    "    print(\"%s Time: %0.2f\" % (k, time_spend.total_seconds()))\n",
    "    \n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5样例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "print iris.feature_names\n",
    "# clf = clf.fit(iris.data, iris.target)\n",
    "# # from sklearn.externals.six import StringIO  \n",
    "# # import pydot \n",
    "# # dot_data = StringIO() \n",
    "# # tree.export_graphviz(clf, out_file=dot_data) \n",
    "# # graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "# # graph.write_pdf(\"iris.pdf\") \n",
    "# from IPython.display import Image  \n",
    "# dot_data = StringIO()  \n",
    "# tree.export_graphviz(clf, out_file=dot_data,  \n",
    "#                          feature_names=iris.feature_names,  \n",
    "#                          class_names=iris.target_names,  \n",
    "#                          filled=True, rounded=True,  \n",
    "#                          special_characters=True)  \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "# graph.write_png(\"iris.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他降维算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Random 2D projection using a random unitary matrix\n",
    "print(\"Computing random projection\")\n",
    "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
    "X_projected = rp.fit_transform(X)\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Projection on to the first 2 principal components\n",
    "print(\"Computing PCA projection\")\n",
    "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Isomap projection of the digits dataset\n",
    "print(\"Computing Isomap embedding\")\n",
    "n_neighbors = 3\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "plt.scatter(X_iso[:, 0], X_iso[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Locally linear embedding of the digits dataset\n",
    "print(\"Computing LLE embedding\")\n",
    "n_neighbors = 3\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='standard')\n",
    "X_lle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# Modified Locally linear embedding of the digits dataset\n",
    "print(\"Computing modified LLE embedding\")\n",
    "n_neighbors = 3\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='modified')\n",
    "X_mlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_mlle[:, 0], X_mlle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# HLLE embedding of the digits dataset\n",
    "print(\"Computing Hessian LLE embedding\")\n",
    "n_neighbors = 6\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='hessian')\n",
    "X_hlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_hlle[:, 0], X_hlle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# LTSA embedding of the digits dataset\n",
    "print(\"Computing LTSA embedding\")\n",
    "n_neighbors = 100\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='ltsa')\n",
    "X_ltsa = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_ltsa[:, 0], X_ltsa[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# MDS  embedding of the digits dataset\n",
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
    "X_mds = clf.fit_transform(X)\n",
    "print(\"Done. Stress: %f\" % clf.stress_)\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# Random Trees embedding of the digits dataset\n",
    "print(\"Computing Totally Random Trees embedding\")\n",
    "hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n",
    "                                       max_depth=5)\n",
    "X_transformed = hasher.fit_transform(X)\n",
    "pca = decomposition.TruncatedSVD(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_transformed)\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Spectral embedding of the digits dataset\n",
    "print(\"Computing Spectral embedding\")\n",
    "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n",
    "                                      eigen_solver=\"arpack\")\n",
    "X_se = embedder.fit_transform(X)\n",
    "plt.scatter(X_se[:, 0], X_se[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## best\n",
    "#----------------------------------------------------------------------\n",
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
