{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import pandas as pd\n",
    "# 数据库访问\n",
    "import MySQLdb, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取当前路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "#获取脚本文件的当前路径\n",
    "def cur_file_dir():\n",
    "     #获取脚本路径\n",
    "     path = sys.path[0]\n",
    "     #判断为脚本文件还是py2exe编译后的文件，如果是脚本文件，则返回的是脚本的目录，如果是py2exe编译后的文件，则返回的是编译后的文件路径\n",
    "     if os.path.isdir(path):\n",
    "         return path\n",
    "     elif os.path.isfile(path):\n",
    "         return os.path.dirname(path)\n",
    "#打印结果\n",
    "\n",
    "# cur_file_dir = cur_file_dir() + '/'\n",
    "cur_file_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据（user_dim.txt）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 读取数据（user_dim.txt）\n",
    "X = np.loadtxt('/Users/dujiawei/git/UserAnalysis/result/user_dim.txt')\n",
    "\n",
    "##归一化\n",
    "#nrow, _ = X.shape\n",
    "#for row in xrange(nrow):\n",
    "#    X[row] = X[row]/sum(X[row])\n",
    "#X = np.vectorize(lambda x: round(x,2))(X)\n",
    "\n",
    "# Open database connection\n",
    "conn = MySQLdb.connect(host='localhost',user='root',passwd='',db='qyw', charset='utf8')\n",
    "#c = ['yellowgreen', 'lightskyblue', 'lightcoral', 'gold', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE算法降维+DBSCAN聚类（效果最好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1 -1  1  1  1 -1  1  1  1  0  1  1  1 -1  0  0  1  1  0  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  0  1  1  0  0  1  1  0  1  1  1  1  1  1  0  1  1  1  1\n",
      "  1  1  1  0  1  1  1  1  1  0 -1  1  1  1  1  1  1  1  1 -1  1  0  1 -1  1\n",
      " -1  0  1  0  1  0 -1  1 -1 -1 -1  0  0 -1  1 -1  1  0  1 -1  1  1  1 -1 -1\n",
      "  0  0 -1 -1  1  1  1  0  1  1  1 -1  1  1  1  1 -1  1  1  1]\n",
      "2\n",
      "98\n",
      "(120, 2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dujiawei/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n",
      "/Users/dujiawei/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/generic.py:1003: FutureWarning: The 'mysql' flavor with DBAPI connection is deprecated and will be removed in future versions. MySQL will be further supported with SQLAlchemy connectables.\n",
      "  dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(120, 2)\n"
     ]
    }
   ],
   "source": [
    "## t-SNE算法降维+DBSCAN聚类（效果最好）\n",
    "# ###############################################################################\n",
    "# not good\n",
    "# Compute Kmeans\n",
    "# Visualize the results on PCA-reduced data\n",
    "#tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "#reduced_data = tsne.fit_transform(X)\n",
    "#plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "#plt.savefig(cur_file_dir+'result/'+'user_dr_tsne.png')\n",
    "## plt.show()\n",
    "#plt.cla()\n",
    "#plt.clf()\n",
    "#plt.close()\n",
    "## compute K-means\n",
    "#kmeans = KMeans(init='k-means++', n_clusters=4, n_init=1)\n",
    "#cls = kmeans.fit(reduced_data)\n",
    "#print cls.cluster_centers_\n",
    "#print cls.labels_\n",
    "#print cls.inertia_\n",
    "#labels = cls.labels_\n",
    "\n",
    "###############################################################################\n",
    "## Visualize the results on PCA-reduced data\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "reduced_data = tsne.fit_transform(X)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "plt.savefig(cur_file_dir+'result/'+'user_dr_tsne.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# Compute DBSCAN\n",
    "#D = distance.squareform(distance.pdist(X)) # 高维数据\n",
    "D = distance.squareform(distance.pdist(reduced_data)) # 低维数据\n",
    "D = np.sort(D,axis=0)\n",
    "minPts = 10\n",
    "nearest = D[1:(minPts+1), :]\n",
    "nearest = nearest.reshape(1, nearest.size)\n",
    "sort_nearest = np.sort(nearest)\n",
    "plt.plot(range(len(sort_nearest[0,:])), sort_nearest[0,:], linewidth=1.0, marker='x')\n",
    "#plt.axis([-2, len(sort_nearest[0,:])+1000, -2, max(sort_nearest[0,:])+2])\n",
    "plt.savefig(cur_file_dir+'result/'+'nearest.png')\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "#db = DBSCAN(eps=0.90, min_samples=minPts).fit(X) # 高维数据\n",
    "db = DBSCAN(eps=30, min_samples=minPts).fit(reduced_data) # 低维数据\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print labels\n",
    "print n_clusters_\n",
    "print len(labels[labels>=0])\n",
    "\n",
    "## 散点图\n",
    "#colors = [c[int(i) % len(c)] for i in labels]\n",
    "#colors = labels\n",
    "#plt.scatter(reduced_data[:, 0], reduced_data[:, 1], 20, colors) # 20为散点的直径\n",
    "\n",
    "## 图形展示\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0,1,len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k  == -1:\n",
    "        # 噪声显示为黑色\n",
    "        col = 'k'\n",
    "        markersize = 3\n",
    "    else:\n",
    "        markersize = 8\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = reduced_data[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,   \n",
    "             markeredgecolor='k', markersize=markersize)   \n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.savefig(cur_file_dir+'result/'+'user_cluster_dbscan.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "## 展示用户聚类结果（散点图和excel表格）\n",
    "USERS = np.loadtxt('/Users/dujiawei/git/UserAnalysis/result/user.txt')\n",
    "USERS_CLS =  np.hstack((USERS.reshape(USERS.size, 1),labels.reshape(labels.size,1)))\n",
    "print USERS_CLS.shape\n",
    "np.savetxt(cur_file_dir+'result/user_cls.txt', USERS_CLS, fmt='%d')\n",
    "USERS_CLS_DF = pd.DataFrame(USERS_CLS, columns=['USER_ID', 'CLS_ID'])\n",
    "print USERS_CLS_DF.shape\n",
    "USERS_CLS_DF.to_sql('qyw_7th_user_clusters', conn, flavor='mysql', if_exists='replace', index=False)\n",
    "USERS_CLS_CNTCASE = pd.read_sql('''\n",
    "    SELECT t1.*,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT USER_ID,\n",
    "          COUNT(DISTINCT CASE_ID) AS CNT_CASE\n",
    "   FROM qyw_7th_yy_succ_all_selected\n",
    "   GROUP BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters) AS t2 ON t1.USER_ID = t2.USER_ID\n",
    "ORDER BY t1.CNT_CASE DESC;\n",
    "''', con=conn)\n",
    "#colors = [c[int(i) % len(c)] for i in np.array(USERS_CLS_CNTCASE['CLS_ID'])]\n",
    "#colors = USERS_CLS_CNTCASE['CLS_ID']\n",
    "#plt.scatter(USERS_CLS_CNTCASE['CNT_CASE'], USERS_CLS_CNTCASE['USER_ID'], 20, colors) # 20为散点的直径\n",
    "## 图形展示\n",
    "labels = np.array(USERS_CLS_CNTCASE['CLS_ID'])\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0,1,len(unique_labels)))\n",
    "cntcases = np.array(USERS_CLS_CNTCASE['CNT_CASE'])\n",
    "cntcases = cntcases.reshape(cntcases.size, 1)\n",
    "uids = np.array(USERS_CLS_CNTCASE['USER_ID'])\n",
    "uids = uids.reshape(uids.size, 1)\n",
    "data = np.hstack((cntcases, uids))\n",
    "plt.axis([min(cntcases)-2, max(cntcases)+2, min(uids)-1000000, max(uids)+1000000]) #hard code, +- 为x轴、y轴分割（一格）单位\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k  == -1:\n",
    "        # 噪声显示为黑色\n",
    "        col = 'k'\n",
    "        markersize = 3\n",
    "    else:\n",
    "        markersize = 8\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = data[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,   \n",
    "             markeredgecolor='k', markersize=markersize) \n",
    "plt.savefig(cur_file_dir+'result/'+'user_cls_cntcase.png')\n",
    "# plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "USERS_CLS_CNTCASE.to_excel(cur_file_dir+'result/'+'user_cls_cntcase.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 年龄分布（年龄映射为标称属性）optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AGE_ALL = pd.read_sql('''\n",
    "SELECT AGE FROM qyw_7th_user;\n",
    "''',con=conn)\n",
    "AGE_ALL = AGE_ALL.AGE\n",
    "AGE_ALL = AGE_ALL.fillna(-1)\n",
    "AGE_ALL = AGE_ALL.sort_values()\n",
    "plt.scatter(range(AGE_ALL.size),AGE_ALL)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取（分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGE=-10 TO 0', 'AGE=0 TO 10', 'AGE=20 TO 30', 'AGE=30 TO 40', 'AGE=40 TO 50', 'AGE=50 TO 60', 'AGE=60 TO 70', 'AGE=70 TO 80', 'AGE=80 TO 90', 'CITY=HOSPITAL LOCATED', 'CITY=OTHERS', 'GENDER=FEMALE', 'GENDER=MALE', 'GENDER=UNKNOWN', 'USER_TYPE=MEDICAL_GUIDED', 'USER_TYPE=SELF_GUIDED']\n",
      "['cls #0', 'cls #1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_INFO_CLS = pd.read_sql('''\n",
    "SELECT t1.GENDER,\n",
    "       t1.MEDICAL_GUIDE AS USER_TYPE,\n",
    "       t1.CITY,\n",
    "       t1.AGE,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user\n",
    "   ORDER BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters\n",
    "   WHERE CLS_ID >= 0\n",
    "   ORDER BY USER_ID) AS t2 ON t1.USER_ID = t2.USER_ID;''', con=conn)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 0, 'GENDER'] = 'UNKNOWN'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 1, 'GENDER'] = 'MALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 2, 'GENDER'] = 'FEMALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] != '', 'USER_TYPE'] = 'MEDICAL_GUIDED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] == '', 'USER_TYPE'] = 'SELF_GUIDED'\n",
    "USER_INFO_CLS['AGE'].fillna(-1, inplace=True)\n",
    "USER_INFO_CLS['CITY'].fillna(u'未知', inplace=True)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] != u'武汉', 'CITY'] = 'HOSPITAL LOCATED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] == u'武汉', 'CITY'] = 'OTHERS'\n",
    "\n",
    "# 年龄离散化（optional）\n",
    "USER_INFO_CLS.AGE = USER_INFO_CLS.AGE.apply(lambda x:'{begin} TO {end}'.format(begin=int(x)/10*10,end=(int(x)/10+1)*10))\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['AGE'] == '-10 -> 0', 'AGE'] = 'UNKNOWN'\n",
    "\n",
    "# encoding\n",
    "USER_INFO_CLS_FEA = USER_INFO_CLS.drop(['CLS_ID'], axis=1)\n",
    "USER_INFO_CLS_FEA_DICT = USER_INFO_CLS_FEA.T.to_dict().values()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "USER_INFO_CLS_DATA = vec.fit_transform(USER_INFO_CLS_FEA_DICT).toarray()\n",
    "USER_INFO_CLS_FEA_NAMES = vec.get_feature_names()\n",
    "print USER_INFO_CLS_FEA_NAMES\n",
    "USER_INFO_CLS_TARGET = USER_INFO_CLS.CLS_ID.values\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(USER_INFO_CLS_DATA, USER_INFO_CLS_TARGET)\n",
    "\n",
    "from sklearn.externals.six import StringIO \n",
    "import pydot \n",
    "\n",
    "# dot_data = StringIO() \n",
    "# tree.export_graphviz(clf, out_file=dot_data) \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "# graph.write_png(cur_file_dir+\"result/USER_INFO_CLS_clf_1.png\") \n",
    "\n",
    "USER_INFO_CLS_TARGET = ['cls #'+str(int(i)) for i in np.unique(USER_INFO_CLS_TARGET)]\n",
    "print USER_INFO_CLS_TARGET\n",
    "from IPython.display import Image  \n",
    "dot_data = StringIO()  \n",
    "tree.export_graphviz(clf, out_file=dot_data,  \n",
    "                         feature_names=USER_INFO_CLS_FEA_NAMES,  \n",
    "                         class_names=USER_INFO_CLS_TARGET,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=False)  # (default=False) When set to False, ignore special characters for PostScript compatibility.\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png(cur_file_dir+\"result/user_info_cls_clf.png\")\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取（预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 25\n",
      "----svm_c_rbf----\n",
      "svm_c_rbf Score: 0.80\n",
      "svm_c_rbf Cross Avg. Score: 0.76 (+/- 0.16)\n",
      "svm_c_rbf Time: 0.01\n",
      "----tree----\n",
      "tree Score: 0.72\n",
      "tree Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "tree Time: 0.01\n",
      "----forest_10----\n",
      "forest_10 Score: 0.80\n",
      "forest_10 Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "forest_10 Time: 0.14\n",
      "----forest_100----\n",
      "forest_100 Score: 0.80\n",
      "forest_100 Cross Avg. Score: 0.80 (+/- 0.25)\n",
      "forest_100 Time: 1.35\n",
      "----svm_linear----\n",
      "svm_linear Score: 0.80\n",
      "svm_linear Cross Avg. Score: 0.80 (+/- 0.25)\n",
      "svm_linear Time: 0.02\n",
      "----bayes----\n",
      "bayes Score: 0.76\n",
      "bayes Cross Avg. Score: 0.68 (+/- 0.41)\n",
      "bayes Time: 0.01\n",
      "----svm_c_linear----\n",
      "svm_c_linear Score: 0.80\n",
      "svm_c_linear Cross Avg. Score: 0.76 (+/- 0.30)\n",
      "svm_c_linear Time: 0.02\n"
     ]
    }
   ],
   "source": [
    "USER_INFO_CLS = pd.read_sql('''\n",
    "SELECT t1.GENDER,\n",
    "       t1.MEDICAL_GUIDE AS USER_TYPE,\n",
    "       t1.CITY,\n",
    "       t1.AGE,\n",
    "       t2.CLS_ID\n",
    "FROM\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user\n",
    "   ORDER BY USER_ID) AS t1\n",
    "INNER JOIN\n",
    "  (SELECT *\n",
    "   FROM qyw_7th_user_clusters\n",
    "   WHERE CLS_ID >= 0\n",
    "   ORDER BY USER_ID) AS t2 ON t1.USER_ID = t2.USER_ID;''', con=conn)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 0, 'GENDER'] = 'UNKNOWN'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 1, 'GENDER'] = 'MALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['GENDER'] == 2, 'GENDER'] = 'FEMALE'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] != '', 'USER_TYPE'] = 'MEDICAL_GUIDED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['USER_TYPE'] == '', 'USER_TYPE'] = 'SELF_GUIDED'\n",
    "USER_INFO_CLS['AGE'].fillna(-1, inplace=True)\n",
    "USER_INFO_CLS['CITY'].fillna(u'未知', inplace=True)\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] != u'武汉', 'CITY'] = 'HOSPITAL LOCATED'\n",
    "USER_INFO_CLS.loc[USER_INFO_CLS['CITY'] == u'武汉', 'CITY'] = 'OTHERS'\n",
    "\n",
    "# # 年龄离散化（optional）\n",
    "# USER_INFO_CLS.AGE = USER_INFO_CLS.AGE.apply(lambda x:'{begin} TO {end}'.format(begin=int(x)/10*10,end=(int(x)/10+1)*10))\n",
    "# USER_INFO_CLS.loc[USER_INFO_CLS['AGE'] == '-10 -> 0', 'AGE'] = 'UNKNOWN'\n",
    "\n",
    "# encoding\n",
    "USER_INFO_CLS_FEA = USER_INFO_CLS.drop(['CLS_ID'], axis=1)\n",
    "USER_INFO_CLS_FEA_DICT = USER_INFO_CLS_FEA.T.to_dict().values()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "USER_INFO_CLS_DATA = vec.fit_transform(USER_INFO_CLS_FEA_DICT).toarray()\n",
    "USER_INFO_CLS_FEA_NAMES = vec.get_feature_names()\n",
    "# print USER_INFO_CLS_FEA_NAMES\n",
    "USER_INFO_CLS_TARGET = USER_INFO_CLS.CLS_ID.values\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(USER_INFO_CLS_DATA, USER_INFO_CLS_TARGET)\n",
    "print len(data_train), len(data_test)\n",
    "\n",
    "# 这里选择朴素贝叶斯、决策树、随机森林和SVM来做一个对比。\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import datetime\n",
    "estimators = {}\n",
    "estimators['bayes'] = GaussianNB()\n",
    "estimators['tree'] = tree.DecisionTreeClassifier()\n",
    "estimators['forest_100'] = RandomForestClassifier(n_estimators = 100)\n",
    "estimators['forest_10'] = RandomForestClassifier(n_estimators = 10)\n",
    "estimators['svm_c_rbf'] = svm.SVC()\n",
    "estimators['svm_c_linear'] = svm.SVC(kernel='linear')\n",
    "estimators['svm_linear'] = svm.LinearSVC()\n",
    "# estimators['svm_nusvc'] = svm.NuSVC() # error, because: http://stackoverflow.com/questions/26987248/nu-is-infeasible\n",
    "\n",
    "for k in estimators.keys():\n",
    "    start_time = datetime.datetime.now()\n",
    "    print '----%s----' % k\n",
    "    estimators[k] = estimators[k].fit(data_train, target_train)\n",
    "    pred = estimators[k].predict(data_test)\n",
    "    # print target_test vs. pred\n",
    "#     print target_test\n",
    "#     print 'vs. '\n",
    "#     print pred\n",
    "    # This is predict score\n",
    "    print(\"%s Score: %0.2f\" % (k, estimators[k].score(data_test, target_test)))\n",
    "    scores = cross_validation.cross_val_score(estimators[k], data_test, target_test, cv=5)\n",
    "    print(\"%s Cross Avg. Score: %0.2f (+/- %0.2f)\" % (k, scores.mean(), scores.std() * 2))\n",
    "#     print(\"%s Pred Score: %0.2f\" % (k, ((float)(len(target_test[target_test == pred])))/len(target_test)))\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_spend = end_time - start_time\n",
    "    print(\"%s Time: %0.2f\" % (k, time_spend.total_seconds()))\n",
    "    \n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5样例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "print iris.feature_names\n",
    "# clf = clf.fit(iris.data, iris.target)\n",
    "# # from sklearn.externals.six import StringIO  \n",
    "# # import pydot \n",
    "# # dot_data = StringIO() \n",
    "# # tree.export_graphviz(clf, out_file=dot_data) \n",
    "# # graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "# # graph.write_pdf(\"iris.pdf\") \n",
    "# from IPython.display import Image  \n",
    "# dot_data = StringIO()  \n",
    "# tree.export_graphviz(clf, out_file=dot_data,  \n",
    "#                          feature_names=iris.feature_names,  \n",
    "#                          class_names=iris.target_names,  \n",
    "#                          filled=True, rounded=True,  \n",
    "#                          special_characters=True)  \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "# graph.write_png(\"iris.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他降维算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Random 2D projection using a random unitary matrix\n",
    "print(\"Computing random projection\")\n",
    "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
    "X_projected = rp.fit_transform(X)\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Projection on to the first 2 principal components\n",
    "print(\"Computing PCA projection\")\n",
    "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Isomap projection of the digits dataset\n",
    "print(\"Computing Isomap embedding\")\n",
    "n_neighbors = 3\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "plt.scatter(X_iso[:, 0], X_iso[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Locally linear embedding of the digits dataset\n",
    "print(\"Computing LLE embedding\")\n",
    "n_neighbors = 3\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='standard')\n",
    "X_lle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# Modified Locally linear embedding of the digits dataset\n",
    "print(\"Computing modified LLE embedding\")\n",
    "n_neighbors = 3\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='modified')\n",
    "X_mlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_mlle[:, 0], X_mlle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# HLLE embedding of the digits dataset\n",
    "print(\"Computing Hessian LLE embedding\")\n",
    "n_neighbors = 6\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='hessian')\n",
    "X_hlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_hlle[:, 0], X_hlle[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# LTSA embedding of the digits dataset\n",
    "print(\"Computing LTSA embedding\")\n",
    "n_neighbors = 100\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='ltsa')\n",
    "X_ltsa = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plt.scatter(X_ltsa[:, 0], X_ltsa[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# MDS  embedding of the digits dataset\n",
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
    "X_mds = clf.fit_transform(X)\n",
    "print(\"Done. Stress: %f\" % clf.stress_)\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## good\n",
    "#----------------------------------------------------------------------\n",
    "# Random Trees embedding of the digits dataset\n",
    "print(\"Computing Totally Random Trees embedding\")\n",
    "hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n",
    "                                       max_depth=5)\n",
    "X_transformed = hasher.fit_transform(X)\n",
    "pca = decomposition.TruncatedSVD(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_transformed)\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Spectral embedding of the digits dataset\n",
    "print(\"Computing Spectral embedding\")\n",
    "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n",
    "                                      eigen_solver=\"arpack\")\n",
    "X_se = embedder.fit_transform(X)\n",
    "plt.scatter(X_se[:, 0], X_se[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## best\n",
    "#----------------------------------------------------------------------\n",
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
